{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_sql import get_schema, Schema, get_sql\n",
    "from evaluation import build_valid_col_units, rebuild_sql_val, rebuild_sql_col, build_foreign_key_map_from_json, eval_exec_match, Evaluator\n",
    "import os, nltk\n",
    "nltk.download('punkt')\n",
    "table = \"table.json\"\n",
    "kmaps = build_foreign_key_map_from_json(table)\n",
    "db_dir = \"database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(label_str, pred_str, label_dbname, db_dir, kmaps):\n",
    "    all = 0\n",
    "    execute = 0\n",
    "    exact = 0\n",
    "    for p, g, db_name in zip(pred_str, label_str, label_dbname):\n",
    "        p_str = p\n",
    "        g_str = g\n",
    "        db = os.path.join(db_dir, db_name, db_name + \".sqlite\")\n",
    "        schema = Schema(get_schema(db)) # schema is a dict with table name as key and list of column names as value\n",
    "        g_sql = get_sql(schema, g_str)\n",
    "        all += 1.0\n",
    "\n",
    "        try:\n",
    "            p_sql = get_sql(schema, p_str)\n",
    "        except:\n",
    "            # If p_sql is not valid, then we will use an empty sql to evaluate with the correct sql\n",
    "            p_sql = {\n",
    "            \"except\": None,\n",
    "            \"from\": {\n",
    "                \"conds\": [],\n",
    "                \"table_units\": []\n",
    "            },\n",
    "            \"groupBy\": [],\n",
    "            \"having\": [],\n",
    "            \"intersect\": None,\n",
    "            \"limit\": None,\n",
    "            \"orderBy\": [],\n",
    "            \"select\": [\n",
    "                False,\n",
    "                []\n",
    "            ],\n",
    "            \"union\": None,\n",
    "            \"where\": []\n",
    "            }\n",
    "\n",
    "        # rebuild sql for value evaluation\n",
    "        kmap = kmaps[db_name]\n",
    "        g_valid_col_units = build_valid_col_units(g_sql['from']['table_units'], schema)\n",
    "        g_sql = rebuild_sql_val(g_sql)\n",
    "        g_sql = rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "        p_valid_col_units = build_valid_col_units(p_sql['from']['table_units'], schema)\n",
    "        p_sql = rebuild_sql_val(p_sql)\n",
    "        p_sql = rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "\n",
    "        exec_score = eval_exec_match(db, p_str, g_str, p_sql, g_sql)\n",
    "        if exec_score:\n",
    "            corr += 1.0\n",
    "\n",
    "        evaluator = Evaluator()\n",
    "        exact_score = evaluator.eval_exact_match(p_sql, g_sql)\n",
    "        if exact_score:\n",
    "            exact += 1.0\n",
    "\n",
    "    return {\"execute\":execute/all, \"exact\": exact/all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_metrics function should be like this:\n",
    "'''\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    label_dbname = pred.label_dbname # list of names of databases\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    output = evaluate(label_str, pred_str, label_dbname, db_dir, kmaps)\n",
    "\n",
    "    return {\n",
    "        \"execution_accuracy\": round(output[\"execute\"], 4),\n",
    "        \"exact_matching\": round(output[\"exact\"], 4),\n",
    "    }\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
